{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Callback to Continue or Stop Training F1score=100%",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrishPatel5611/Surface_Defect_Detection/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "riffatsiddiqui_ceramic_tiles_defects_crackspotspinhole_path = kagglehub.dataset_download('riffatsiddiqui/ceramic-tiles-defects-crackspotspinhole')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "hp8PcPtnDprs"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout, MaxPooling2D,BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import numpy as np\n",
        "import time\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "print ('modules loaded')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:08:46.440466Z",
          "iopub.execute_input": "2022-01-14T04:08:46.440755Z",
          "iopub.status.idle": "2022-01-14T04:08:46.457457Z",
          "shell.execute_reply.started": "2022-01-14T04:08:46.440725Z",
          "shell.execute_reply": "2022-01-14T04:08:46.456185Z"
        },
        "trusted": true,
        "id": "gDsUEdykDprt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdir=r'../input/ceramic-tiles-defects-crackspotspinhole/dataset'\n",
        "flist=os.listdir(sdir)\n",
        "labels=[]\n",
        "filepaths=[]\n",
        "for f in flist:\n",
        "    fpath=os.path.join(sdir,f)\n",
        "    filepaths.append(fpath)\n",
        "    klass =f.split('.')[0]\n",
        "    labels.append(klass)\n",
        "Fseries=pd.Series(filepaths, name='filepaths')\n",
        "Lseries = pd.Series(labels, name='labels')\n",
        "df=pd.concat([Fseries, Lseries], axis=1)\n",
        "print (df['labels'].value_counts())\n",
        "print(df.isna().sum())\n",
        "classes=list(df['labels'].unique())\n",
        "class_count=len(classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:08:46.45978Z",
          "iopub.execute_input": "2022-01-14T04:08:46.460226Z",
          "iopub.status.idle": "2022-01-14T04:08:46.524772Z",
          "shell.execute_reply.started": "2022-01-14T04:08:46.460179Z",
          "shell.execute_reply": "2022-01-14T04:08:46.523755Z"
        },
        "trusted": true,
        "id": "hqJkSBytDpru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split df into a train_df, a test_df and a valid df"
      ],
      "metadata": {
        "id": "hExhDMUoDpru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, dummy_df =train_test_split(df, train_size=.9, shuffle=True, random_state=123, stratify= df['labels'])\n",
        "valid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify =dummy_df['labels'])\n",
        "print('train_df length: ', len(train_df), '  test_df length: ',len(test_df), '  valid_df length: ', len(valid_df))\n",
        "print (train_df['labels'].value_counts())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:08:46.526537Z",
          "iopub.execute_input": "2022-01-14T04:08:46.527112Z",
          "iopub.status.idle": "2022-01-14T04:08:46.568076Z",
          "shell.execute_reply.started": "2022-01-14T04:08:46.527067Z",
          "shell.execute_reply": "2022-01-14T04:08:46.566977Z"
        },
        "trusted": true,
        "id": "Kdc98XmyDprv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_df is not balanced but should be OK"
      ],
      "metadata": {
        "id": "bkv-2z0iDprv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## input an image and get the shape"
      ],
      "metadata": {
        "id": "ZxhcCT2xDprv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path=r'../input/ceramic-tiles-defects-crackspotspinhole/dataset/crack.1.jpg'\n",
        "img=plt.imread(img_path)\n",
        "print ('Image shape is: ', img.shape)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:08:46.570745Z",
          "iopub.execute_input": "2022-01-14T04:08:46.571179Z",
          "iopub.status.idle": "2022-01-14T04:08:46.711527Z",
          "shell.execute_reply.started": "2022-01-14T04:08:46.571131Z",
          "shell.execute_reply": "2022-01-14T04:08:46.710274Z"
        },
        "trusted": true,
        "id": "JoQJI8O2Dprw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## make train, test and valid generators"
      ],
      "metadata": {
        "id": "YWhbhFU-Dprw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_size=(150,150)\n",
        "batch_size= 40\n",
        "# calculate test_batch_size and test_step so we go through test files exactly once\n",
        "length=len(test_df)\n",
        "test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]\n",
        "test_steps=int(length/test_batch_size)\n",
        "print ('test batch size= ', test_batch_size, '  test steps= ', test_steps)\n",
        "trgen=ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
        "tvgen=ImageDataGenerator()\n",
        "train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
        "valid_gen=tvgen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
        "test_gen=tvgen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:08:46.713711Z",
          "iopub.execute_input": "2022-01-14T04:08:46.714045Z",
          "iopub.status.idle": "2022-01-14T04:08:48.420785Z",
          "shell.execute_reply.started": "2022-01-14T04:08:46.714Z",
          "shell.execute_reply": "2022-01-14T04:08:48.419862Z"
        },
        "trusted": true,
        "id": "MRW_8sYODprw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use transfer learning with EfficientNetB3 model"
      ],
      "metadata": {
        "id": "wRMOnkxvDprw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape=(img_size[0], img_size[1], 3)\n",
        "model_name='EfficientNetB3'\n",
        "base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
        "x=base_model.output\n",
        "x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
        "x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
        "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
        "x=Dropout(rate=.45, seed=123)(x)\n",
        "output=Dense(class_count, activation='softmax')(x)\n",
        "model=Model(inputs=base_model.input, outputs=output)\n",
        "model.compile(Adamax(learning_rate=.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:08:48.422409Z",
          "iopub.execute_input": "2022-01-14T04:08:48.422829Z",
          "iopub.status.idle": "2022-01-14T04:08:51.61075Z",
          "shell.execute_reply.started": "2022-01-14T04:08:48.422783Z",
          "shell.execute_reply": "2022-01-14T04:08:51.609738Z"
        },
        "trusted": true,
        "id": "SJh9kBOlDprw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define a custom callback that after ask_epoch asks if you want to continue training or halt.\n",
        "## You can enter an integer for how many more\n",
        "## epochs to run then be asked again, or enter H to halt training"
      ],
      "metadata": {
        "id": "WjOuGMeYDprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ASK(keras.callbacks.Callback):\n",
        "    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n",
        "        super(ASK, self).__init__()\n",
        "        self.model=model\n",
        "        self.ask_epoch=ask_epoch\n",
        "        self.epochs=epochs\n",
        "        self.ask=True # if True query the user on a specified epoch\n",
        "\n",
        "    def on_train_begin(self, logs=None): # this runs on the beginning of training\n",
        "        if self.ask_epoch == 0:\n",
        "            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n",
        "            self.ask_epoch=1\n",
        "        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n",
        "            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n",
        "            self.ask=False # do not query the user\n",
        "        if self.epochs == 1:\n",
        "            self.ask=False # running only for 1 epoch so do not query user\n",
        "        else:\n",
        "            print('Training will proceed until epoch', ask_epoch,' then you will be asked to')\n",
        "            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')\n",
        "        self.start_time= time.time() # set the time at which training started\n",
        "\n",
        "    def on_train_end(self, logs=None):   # runs at the end of training\n",
        "        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted\n",
        "        hours = tr_duration // 3600\n",
        "        minutes = (tr_duration - (hours * 3600)) // 60\n",
        "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
        "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
        "        print (msg, flush=True) # print out training duration time\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
        "        if self.ask: # are the conditions right to query the user?\n",
        "            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n",
        "                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n",
        "                ans=input()\n",
        "\n",
        "                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n",
        "                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n",
        "                    self.model.stop_training = True # halt training\n",
        "                else: # user wants to continue training\n",
        "                    self.ask_epoch += int(ans)\n",
        "                    if self.ask_epoch > self.epochs:\n",
        "                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush=True)\n",
        "\n",
        "                    else:\n",
        "                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:08:51.61382Z",
          "iopub.execute_input": "2022-01-14T04:08:51.614203Z",
          "iopub.status.idle": "2022-01-14T04:08:51.632218Z",
          "shell.execute_reply.started": "2022-01-14T04:08:51.614129Z",
          "shell.execute_reply": "2022-01-14T04:08:51.63071Z"
        },
        "trusted": true,
        "id": "NxbRNonMDprx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define an early stop callback and a reduce learning rate callback and instantiate the ASK callback"
      ],
      "metadata": {
        "id": "HIAkJFAxDprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5,   patience=1,  verbose=1)\n",
        "estop=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\",   patience=4,  verbose=1,   restore_best_weights=True)\n",
        "epochs=40\n",
        "ask_epoch=2 # at end of 5th epoch ask to enter H to halt training or an integer for how many more epochs to run then ask again\n",
        "callbacks=[rlronp, estop, ASK( model,epochs, ask_epoch)]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:08:51.634398Z",
          "iopub.execute_input": "2022-01-14T04:08:51.63484Z",
          "iopub.status.idle": "2022-01-14T04:08:51.650219Z",
          "shell.execute_reply.started": "2022-01-14T04:08:51.63476Z",
          "shell.execute_reply": "2022-01-14T04:08:51.649095Z"
        },
        "trusted": true,
        "id": "jpp9d_AsDprx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train the model"
      ],
      "metadata": {
        "id": "K7Svb_ruDprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n",
        "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:08:51.651871Z",
          "iopub.execute_input": "2022-01-14T04:08:51.652244Z",
          "iopub.status.idle": "2022-01-14T04:17:11.435927Z",
          "shell.execute_reply.started": "2022-01-14T04:08:51.652194Z",
          "shell.execute_reply": "2022-01-14T04:17:11.435011Z"
        },
        "trusted": true,
        "id": "WfxFuSxaDprx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate model on the test set"
      ],
      "metadata": {
        "id": "sE3ul4pgDprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " acc= model.evaluate(test_gen, verbose= 1,  steps=test_steps)\n",
        " print('Model accuracy on test set is ', acc[1]*100)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:19:40.619794Z",
          "iopub.execute_input": "2022-01-14T04:19:40.620117Z",
          "iopub.status.idle": "2022-01-14T04:19:43.279963Z",
          "shell.execute_reply.started": "2022-01-14T04:19:40.620085Z",
          "shell.execute_reply": "2022-01-14T04:19:43.278275Z"
        },
        "trusted": true,
        "id": "AgsAnUpkDprx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": [
        "## save the model"
      ],
      "metadata": {
        "id": "YZ2KWRlpDprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "working_dir=r'./'\n",
        "save_path=os.path.join(working_dir, 'EfficientNetB3.h5')\n",
        "model.save(save_path, overwrite=True, include_optimizer=True, save_format='h5')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:27:10.975578Z",
          "iopub.execute_input": "2022-01-14T04:27:10.975876Z",
          "iopub.status.idle": "2022-01-14T04:27:12.547418Z",
          "shell.execute_reply.started": "2022-01-14T04:27:10.975844Z",
          "shell.execute_reply": "2022-01-14T04:27:12.546348Z"
        },
        "trusted": true,
        "id": "b9iQvt0GDpry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## do predictions on the test set and generate classification report"
      ],
      "metadata": {
        "id": "BZMgfKA9Dpry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "classes=list(train_gen.class_indices.keys())\n",
        "class_count=len(classes)\n",
        "labels=test_gen.labels\n",
        "files=test_gen.filenames\n",
        "error_file_list=[]\n",
        "indexes=[]\n",
        "errors=0\n",
        "preds=model.predict(test_gen, steps=test_steps, verbose=1)\n",
        "tests=len(preds)\n",
        "for i, p in enumerate (preds):\n",
        "    index=np.argmax(p)\n",
        "    indexes.append(index)\n",
        "    if index != labels[i]:\n",
        "        errors +=1\n",
        "        error_file_list.append(files[i])\n",
        "acc=( tests-errors)/tests * 100\n",
        "print(f'There were {errors}, errors in {tests} tests for an accuracy of {acc:6.2f}' )\n",
        "if errors > 0:\n",
        "    print ('A list of files that were incorrectly predicted is shown below')\n",
        "    for i in range (len(error_file_list)):\n",
        "        print (error_file_list[i])\n",
        "\n",
        "clr = classification_report(labels, indexes, target_names=classes, digits= 4)\n",
        "print(\"Classification Report:\\n----------------------\\n\", clr)\n",
        "cm = confusion_matrix(labels, indexes )\n",
        "length=len(classes)\n",
        "if length<8:\n",
        "    fig_width=8\n",
        "    fig_height=8\n",
        "else:\n",
        "    fig_width= int(length * .5)\n",
        "    fig_height= int(length * .5)\n",
        "plt.figure(figsize=(fig_width, fig_height))\n",
        "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n",
        "plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n",
        "plt.yticks(np.arange(length)+.5, classes, rotation=0)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T05:04:14.371157Z",
          "iopub.execute_input": "2022-01-14T05:04:14.3717Z",
          "iopub.status.idle": "2022-01-14T05:04:16.457593Z",
          "shell.execute_reply.started": "2022-01-14T05:04:14.371662Z",
          "shell.execute_reply": "2022-01-14T05:04:16.456644Z"
        },
        "trusted": true,
        "id": "nBVeJ4K8Dpry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to do predictions on a single image file"
      ],
      "metadata": {
        "id": "mVyHDFlhDpry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "img=plt.imread(img_path) # read in the image shown earlier\n",
        "print ('Input image shape is ', img.shape)\n",
        "# resize the image so it is the same size as the images the model was trained on\n",
        "img=cv2.resize(img, img_size) # in earlier code img_size=(150,150) was used for training the model\n",
        "print ('the resized image has shape ', img.shape)\n",
        "### show the resized image\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "# Normally the next line of code rescales the images. However the EfficientNet model expects images in the range 0 to 255\n",
        "# img= img/255\n",
        "# plt.imread returns a numpy array so it is not necessary to convert the image to a numpy array\n",
        "# since we have only one image we have to expand the dimensions of img so it is off the form (1,150,150,3)\n",
        "# where the first dimension 1 is the batch size used by model.predict\n",
        "img=np.expand_dims(img, axis=0)\n",
        "print ('image shape after expanding dimensions is ',img.shape)\n",
        "# now predict the image\n",
        "pred=model.predict(img)\n",
        "print ('the shape of prediction is ', pred.shape)\n",
        "# this dataset has 15 classes so model.predict will return a list of 15 probability values\n",
        "# we want to find the index of the column that has the highest probability\n",
        "index=np.argmax(pred[0])\n",
        "# to get the actual Name of the class earlier Imade a list of the class names called classes\n",
        "klass=classes[index]\n",
        "# lets get the value of the highest probability\n",
        "probability=pred[0][index]*100\n",
        "# print out the class, and the probability\n",
        "print(f'the image is predicted as being {klass} with a probability of {probability:6.2f} %')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-14T04:51:52.839446Z",
          "iopub.execute_input": "2022-01-14T04:51:52.83975Z",
          "iopub.status.idle": "2022-01-14T04:51:55.726403Z",
          "shell.execute_reply.started": "2022-01-14T04:51:52.83972Z",
          "shell.execute_reply": "2022-01-14T04:51:55.725224Z"
        },
        "trusted": true,
        "id": "InYEyK5mDpry"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}